{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbSWZSVei_li",
        "outputId": "694a176b-6446-4ae8-fb1a-36614dd2aeed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting talos\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/90/2455415b2a2170ad649b66d79ea74ff1af546c012836a2b621323a5fabfd/talos-1.0-py3-none-any.whl (53kB)\n",
            "\r\u001b[K     |██████                          | 10kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 20kB 28.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 30kB 33.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 40kB 25.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 51kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from talos) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from talos) (2.23.0)\n",
            "Collecting wrangle\n",
            "  Downloading https://files.pythonhosted.org/packages/85/35/bc729e377417613f2d062a890faea5d649ef1a554df21499e9c3a4a5501a/wrangle-0.6.7.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from talos) (1.19.5)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from talos) (0.0)\n",
            "Collecting astetik\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/c1/b40ed42915c66d54728a6059de0d35088b329a677f01e9c6f50a71b5b361/astetik-1.11.1-py3-none-any.whl (5.4MB)\n",
            "\u001b[K     |████████████████████████████████| 5.4MB 14.6MB/s \n",
            "\u001b[?25hCollecting chances\n",
            "  Downloading https://files.pythonhosted.org/packages/fa/d8/d61112d7476dc3074b855f1edd8556cde9b49b7106853f0b060109dd4c82/chances-0.1.9.tar.gz\n",
            "Collecting kerasplotlib\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/b7/31663d3b5ea9afd8c2c6ffa06d3c4e118ef363e12dc75b7c49fb6a2d22aa/kerasplotlib-0.1.6.tar.gz\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from talos) (1.1.5)\n",
            "Collecting statsmodels>=0.11.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/69/8eef30a6237c54f3c0b524140e2975f4b1eea3489b45eb3339574fc8acee/statsmodels-0.12.2-cp37-cp37m-manylinux1_x86_64.whl (9.5MB)\n",
            "\u001b[K     |████████████████████████████████| 9.5MB 58.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from talos) (2.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->talos) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->talos) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->talos) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->talos) (1.24.3)\n",
            "Collecting scipy==1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/39/066ecde98f373430bf7a39a02d91c7075b01ef4fc928456e8e31577342d6/scipy-1.2.0-cp37-cp37m-manylinux1_x86_64.whl (26.6MB)\n",
            "\u001b[K     |████████████████████████████████| 26.6MB 80.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from wrangle->talos) (2.4.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->talos) (0.22.2.post1)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from astetik->talos) (5.5.0)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.7/dist-packages (from astetik->talos) (0.5.1)\n",
            "Collecting geonamescache\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/ba/b7939087621bfeb24c0f52c4b879865a9f902cda72efd119f4275400e692/geonamescache-1.2.0-py3-none-any.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 45.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from astetik->talos) (0.11.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->talos) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->talos) (2.8.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (2.5.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (1.6.3)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (2.5.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (1.1.2)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (1.34.1)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (1.12.1)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (3.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (0.36.2)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (1.12)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (3.7.4.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (3.12.4)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (1.15.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (1.1.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->talos) (0.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->wrangle->talos) (3.13)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->talos) (1.0.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from IPython->astetik->talos) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->astetik->talos) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython->astetik->talos) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython->astetik->talos) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython->astetik->talos) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->astetik->talos) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython->astetik->talos) (56.1.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->astetik->talos) (5.0.5)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn->astetik->talos) (3.2.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.0.0->talos) (1.30.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.0.0->talos) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.0.0->talos) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.0.0->talos) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.0.0->talos) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.0.0->talos) (3.3.4)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=2.0.0->talos) (1.5.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->IPython->astetik->talos) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->astetik->talos) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->IPython->astetik->talos) (0.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn->astetik->talos) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn->astetik->talos) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn->astetik->talos) (1.3.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.0.0->talos) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.0.0->talos) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.0.0->talos) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.0.0->talos) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.0.0->talos) (4.0.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.0.0->talos) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.0.0->talos) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.0.0->talos) (3.4.1)\n",
            "Building wheels for collected packages: wrangle, chances, kerasplotlib\n",
            "  Building wheel for wrangle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrangle: filename=wrangle-0.6.7-cp37-none-any.whl size=49894 sha256=1cfaadab3029a9302cca97390bfc0502d354c830a3881f3deec73d22d7334a98\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/1b/50/d0403ce6ef269e364894da7b50db68db14c4ac62c577561e2d\n",
            "  Building wheel for chances (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chances: filename=chances-0.1.9-cp37-none-any.whl size=41610 sha256=5089a7273bf42cc65604c785d14ec18204374b7b9f0270bde5c0a811b8125fe3\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/33/46/c871b94249bd57d17797d049b3dff8e3a09c315afb67eb14c6\n",
            "  Building wheel for kerasplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kerasplotlib: filename=kerasplotlib-0.1.6-cp37-none-any.whl size=3603 sha256=52fbb3f5035435c48c73132a46d6a13bac6dc92265b108017a086649a1c2cb92\n",
            "  Stored in directory: /root/.cache/pip/wheels/9d/d3/8c/9503a22b0a38e8b21c70ad834e4606d209193443e5c709305d\n",
            "Successfully built wrangle chances kerasplotlib\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy, statsmodels, wrangle, geonamescache, astetik, chances, kerasplotlib, talos\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Found existing installation: statsmodels 0.10.2\n",
            "    Uninstalling statsmodels-0.10.2:\n",
            "      Successfully uninstalled statsmodels-0.10.2\n",
            "Successfully installed astetik-1.11.1 chances-0.1.9 geonamescache-1.2.0 kerasplotlib-0.1.6 scipy-1.2.0 statsmodels-0.12.2 talos-1.0 wrangle-0.6.7\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install talos\n",
        "import numpy as np, pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "import string\n",
        "from string import ascii_lowercase\n",
        "from tqdm import tqdm_notebook\n",
        "import itertools\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from functools import reduce\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "import talos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICDbB8cH5d3w",
        "outputId": "69010729-5e85-4436-cf74-2a9b64ebf18a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOZzSnydx02a"
      },
      "outputs": [],
      "source": [
        "class Foul_detector():\n",
        "  def __init__(self):\n",
        "        def remove_contraction(self,text):\n",
        "            contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"'cause\": \"because\",\n",
        "                           \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
        "                           \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'll\": \"he will\",\n",
        "                           \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\",\n",
        "                           \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n",
        "                           \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\",\n",
        "                           \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\",\n",
        "                           \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\",\n",
        "                           \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\",\n",
        "                           \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\",\n",
        "                           \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                           \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n",
        "                           \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\",\n",
        "                           \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n",
        "                           \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "                           \"so's\": \"so as\",\n",
        "                           \"this's\": \"this is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\",\n",
        "                           \"that's\": \"that is\",\n",
        "                           \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\",\n",
        "                           \"here's\": \"here is\",\n",
        "                           \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
        "                           \"they'll've\": \"they will have\",\n",
        "                           \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
        "                           \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\",\n",
        "                           \"we'll've\": \"we will have\",\n",
        "                           \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\",\n",
        "                           \"what'll've\": \"what will have\",\n",
        "                           \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
        "                           \"when've\": \"when have\",\n",
        "                           \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\",\n",
        "                           \"who'll\": \"who will\",\n",
        "                           \"who'll've\": \"who will have\",\n",
        "                           \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\",\n",
        "                           \"will've\": \"will have\",\n",
        "                           \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n",
        "                           \"wouldn't\": \"would not\",\n",
        "                           \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "                           \"y'all'd've\": \"you all would have\",\n",
        "                           \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\",\n",
        "                           \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n",
        "                           \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "            specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
        "            for s in specials:\n",
        "                   text = text.replace(s, \"'\")\n",
        "            text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split(\" \")])\n",
        "            return text\n",
        "        def clean_repeat_patterns_lower(self,text,remove_repeat_text=True, remove_patterns_text=True, is_lower=True):\n",
        "\n",
        "              RE_PATTERNS = {' american ':\n",
        "              [\n",
        "              'amerikan'\n",
        "              ],\n",
        "\n",
        "              ' adolf ':\n",
        "               [\n",
        "               'adolf'\n",
        "               ],\n",
        "\n",
        "\n",
        "               ' hitler ':\n",
        "               [\n",
        "               'hitler'\n",
        "               ],\n",
        "\n",
        "               ' fuck':\n",
        "              [\n",
        "              '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n",
        "              '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n",
        "              ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n",
        "              '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n",
        "              'feck ', ' fux ', 'f\\*\\*', 'f**k','fu*k',\n",
        "              'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck'\n",
        "               ],\n",
        "\n",
        "               ' ass ':\n",
        "               [\n",
        "             '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$',\n",
        "             '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n",
        "             'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s','a55', '@$$'\n",
        "              ],\n",
        "\n",
        "              ' ass hole ':\n",
        "              [\n",
        "              ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole', 'a**hole'\n",
        "              ],\n",
        "\n",
        "              ' bitch ':\n",
        "              [\n",
        "              'b[w]*i[t]*ch', 'b!tch',\n",
        "              'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n",
        "               'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h', 'b!tch', 'bi+ch', 'l3itch'\n",
        "              ],\n",
        "\n",
        "              ' bastard ':\n",
        "              [\n",
        "              'ba[s|z]+t[e|a]+rd'\n",
        "              ],\n",
        "\n",
        "             ' trans gender':\n",
        "              [\n",
        "             'transgender'\n",
        "             ],\n",
        "\n",
        "             ' gay ':\n",
        "             [\n",
        "            'gay'\n",
        "             ],\n",
        "\n",
        "             ' cock ':\n",
        "              [\n",
        "            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n",
        "            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n",
        "        ],\n",
        "\n",
        "    ' dick ':\n",
        "        [\n",
        "            ' dick[^aeiou]', 'deek', 'd i c k', 'dik'\n",
        "        ],\n",
        "\n",
        "    ' suck ':\n",
        "        [\n",
        "            'sucker', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'sucks', '5uck', 's u c k'\n",
        "        ],\n",
        "\n",
        "    ' cunt ':\n",
        "        [\n",
        "            'cunt', 'c u n t'\n",
        "        ],\n",
        "\n",
        "    ' bull shit ':\n",
        "        [\n",
        "            'bullsh\\*t', 'bull\\$hit'\n",
        "        ],\n",
        "\n",
        "    ' homo sex ual':\n",
        "        [\n",
        "            'homosexual'\n",
        "        ],\n",
        "\n",
        "    ' jerk ':\n",
        "        [\n",
        "            'jerk'\n",
        "        ],\n",
        "\n",
        "    ' idiot ':\n",
        "        [\n",
        "            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots'\n",
        "                                                                                      'i d i o t'\n",
        "        ],\n",
        "\n",
        "    ' dumb ':\n",
        "        [\n",
        "            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n",
        "        ],\n",
        "\n",
        "    ' shit ':\n",
        "        [\n",
        "            'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t', '$h1t'\n",
        "        ],\n",
        "\n",
        "    ' shit hole ':\n",
        "        [\n",
        "            'shythole'\n",
        "        ],\n",
        "\n",
        "    ' retard ':\n",
        "        [\n",
        "            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n",
        "        ],\n",
        "\n",
        "    ' rape ':\n",
        "        [\n",
        "            ' raped'\n",
        "        ],\n",
        "\n",
        "    ' dumb ass':\n",
        "        [\n",
        "            'dumbass', 'dubass'\n",
        "        ],\n",
        "\n",
        "    ' ass head':\n",
        "        [\n",
        "            'butthead'\n",
        "        ],\n",
        "\n",
        "    ' sex ':\n",
        "        [\n",
        "            'sexy', 's3x', 'sexuality'\n",
        "        ],\n",
        "\n",
        "\n",
        "    ' nigger ':\n",
        "        [\n",
        "            'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n",
        "        ],\n",
        "\n",
        "    ' shut the fuck up':\n",
        "        [\n",
        "            'stfu', 'st*u'\n",
        "        ],\n",
        "\n",
        "    ' pussy ':\n",
        "        [\n",
        "            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses', 'p*ssy'\n",
        "        ],\n",
        "\n",
        "    ' faggot ':\n",
        "        [\n",
        "            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n",
        "            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n",
        "        ],\n",
        "\n",
        "    ' mother fucker':\n",
        "        [\n",
        "            ' motha ', ' motha f', ' mother f', 'motherucker',\n",
        "        ],\n",
        "\n",
        "    ' whore ':\n",
        "        [\n",
        "            'wh\\*\\*\\*', 'w h o r e'\n",
        "        ],\n",
        "    ' fucking ':\n",
        "        [\n",
        "            'f*$%-ing'\n",
        "        ],\n",
        "}\n",
        "\n",
        "              if is_lower:\n",
        "                text=text.lower()\n",
        "              if remove_patterns_text:\n",
        "                   for target, patterns in RE_PATTERNS.items():\n",
        "                         for pat in patterns:\n",
        "                            text=str(text).replace(pat, target)\n",
        "\n",
        "              if remove_repeat_text:\n",
        "                      text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
        "                      text = str(text).replace(\"\\n\", \" \")\n",
        "                      text = re.sub(r'[^\\w\\s]',' ',text)\n",
        "                      text = re.sub('[0-9]',\"\",text)\n",
        "                      text = re.sub(\" +\", \" \", text)\n",
        "                      text = re.sub(\"([^\\x00-\\x7F])+\",\" \",text)\n",
        "                      return text\n",
        "\n",
        "        def remove_emojis(self,text):\n",
        "          emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"\n",
        "        u\"\\U0001F300-\\U0001F5FF\"\n",
        "        u\"\\U0001F680-\\U0001F6FF\"\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "        u\"\\U0001F1F2-\\U0001F1F4\"\n",
        "        u\"\\U0001F1E6-\\U0001F1FF\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U0001F1F2\"\n",
        "        u\"\\U0001F1F4\"\n",
        "        u\"\\U0001F620\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "          return emoji_pattern.sub(r'', text)\n",
        "        def prepare_stopwords_list():\n",
        "            stopword_list=STOP_WORDS\n",
        "            potential_stopwords=['editor', 'reference', 'thank', 'work','find', 'good', 'know', 'like', 'look', 'thing', 'want', 'time', 'list', 'section','wikipedia', 'doe', 'add','new', 'try', 'think', 'write','use', 'user', 'way', 'page']\n",
        "            for word in potential_stopwords:\n",
        "               stopword_list.add(word)\n",
        "            return(stopword_list)\n",
        "\n",
        "        def remove_stop_words(self,text, remove_stop=True):\n",
        "               stop_words=prepare_stopwords_list()\n",
        "               output = \"\"\n",
        "               if remove_stop:\n",
        "                    text=text.split(\" \")\n",
        "                    for word in text:\n",
        "                       if word not in stop_words:\n",
        "                          output=output + \" \" + word\n",
        "               else :\n",
        "                   output=text\n",
        "               return str(output.strip())\n",
        "\n",
        "        def lemmatize(text, lemmatization=True):\n",
        "              wordnet_lemmatizer = WordNetLemmatizer()\n",
        "              output=\"\"\n",
        "              if lemmatization:\n",
        "                  text=text.split(\" \")\n",
        "                  for word in text:\n",
        "                     word1 = wordnet_lemmatizer.lemmatize(word, pos = \"n\")\n",
        "                     word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
        "                     word3 = wordnet_lemmatizer.lemmatize(word2, pos = \"a\")\n",
        "                     word4 = wordnet_lemmatizer.lemmatize(word3, pos = \"r\")\n",
        "                     output=output + \" \" + word4\n",
        "              else:\n",
        "                 output=text\n",
        "              return str(output.strip())\n",
        "        def clean_text(self,text):\n",
        "            text=remove_contraction(text)\n",
        "            text=clean_repeat_patterns_lower(text)\n",
        "            text=remove_emojis(text)\n",
        "            text=remove_stop_words(text)\n",
        "            text=lemmatize(text)\n",
        "            return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFfnCK-r2-b2"
      },
      "outputs": [],
      "source": [
        "max_features=100000\n",
        "maxpadlen = 200\n",
        "val_split = 0.2\n",
        "embedding_dim_fasttext = 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7M7YKV5zybA",
        "outputId": "bfd4578c-954d-4803-ac6e-713a0c5b11ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Words in Vocabulary:  148917\n"
          ]
        }
      ],
      "source": [
        "#print(\"Words in Vocabulary: \",len(word_index))\n",
        "word_index = tokenizer.word_index\n",
        "print(\"Words in Vocabulary: \",len(word_index))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03l-W72Y5Le4"
      },
      "outputs": [],
      "source": [
        "embeddings_index_fasttext = {}\n",
        "f = open('/content/drive/My Drive/wiki-news-300d-1M.vec', encoding='utf8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    embeddings_index_fasttext[word] = np.asarray(values[1:], dtype='float32')\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvaBZ5l66A05",
        "outputId": "ede04af0-fdc6-49f0-839c-31e07104c479"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Completed!\n"
          ]
        }
      ],
      "source": [
        "embedding_matrix_fasttext = np.random.random((len(word_index) + 1, embedding_dim_fasttext))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index_fasttext.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix_fasttext[i] = embedding_vector\n",
        "print(\" Completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kB6o61pIzzj7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJOzWjnZ2GnE"
      },
      "outputs": [],
      "source": [
        "MAX_VOCAB_SIZE = 100000\n",
        "MAX_SEQUENCE_LENGTH = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZRoB1MW1u3V"
      },
      "outputs": [],
      "source": [
        "def toxicity_level(string):\n",
        "    new_string = [string]\n",
        "    new_string = tokenizer.texts_to_sequences(new_string)\n",
        "    new_string = pad_sequences(new_string, maxlen=maxpadlen, padding='post')\n",
        "\n",
        "    prediction = lstm_model.predict(new_string)\n",
        "\n",
        "\n",
        "    print(\"Toxicity levels for '{}':\".format(string))\n",
        "    print('Toxic:         {:.0%}'.format(prediction[0][0]))\n",
        "    #print('Severe Toxic:  {:.0%}'.format(prediction[0][1]))\n",
        "    #print('Obscene:       {:.0%}'.format(prediction[0][2]))\n",
        "    #print('Threat:        {:.0%}'.format(prediction[0][3]))\n",
        "    #print('Insult:        {:.0%}'.format(prediction[0][4]))\n",
        "    #print('Identity Hate: {:.0%}'.format(prediction[0][5]))\n",
        "    #print()\n",
        "\n",
        "\n",
        "\n",
        "    if prediction[0][0] < 0.05:\n",
        "        result = \"No Foul Language Detected\"\n",
        "    if prediction[0][0] >= 0.05:\n",
        "      result = \"Foul Language Detected\"\n",
        "\n",
        "    return result\n",
        "\n",
        "def main():\n",
        "     import pickle\n",
        "     lstm_model = keras.models.load_model('my_model.h5')\n",
        "     with open(\"/content/drive/My Drive/tokenizer.pickle\", 'rb') as handle:\n",
        "            tokenizer = pickle.load(handle)\n",
        "     vectorizer=pickle.load(open('/content/vectorizer1.pkl', 'rb'))\n",
        "     Foul_detector()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "tWdjgnj13YcI",
        "outputId": "56afaa03-5a86-4529-f071-06fcf0c1f7fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Toxicity levels for 'hello,how are you?':\n",
            "Toxic:         0%\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'No Foul Language Detectected'"
            ]
          },
          "execution_count": 83,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "toxicity_level('hello,how are you?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "f1evwG5Hj1tV",
        "outputId": "68745199-ce6e-464e-babd-ae80ec26b695"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Toxicity levels for 'you are stupid?':\n",
            "Toxic:         78%\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Foul'"
            ]
          },
          "execution_count": 84,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "toxicity_level('you are stupid?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "DZJW5YMQk2sc",
        "outputId": "fcbe93a9-a873-4857-bbce-f50689768b0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Toxicity levels for 'Do I love Abimbola?':\n",
            "Toxic:         2%\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'No Foul Language Detected'"
            ]
          },
          "execution_count": 90,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "toxicity_level('Do I love Abimbola?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "IdT4izm3k7u0",
        "outputId": "42256674-4ca2-40fd-80a7-754604136b07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Toxicity levels for 'paraventure,you are silly?':\n",
            "Toxic:         8%\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Foul Language Detected'"
            ]
          },
          "execution_count": 88,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "toxicity_level('paraventure,you are silly?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "-HtcPJc_lnVg",
        "outputId": "484aca93-6f5f-4844-a932-2f4077de7265"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Toxicity levels for 'have a nice day':\n",
            "Toxic:         0%\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'No Foul Language Detected'"
            ]
          },
          "execution_count": 89,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "toxicity_level('have a nice day')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMpd-l3amXCh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
